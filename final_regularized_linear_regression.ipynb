{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvKhknbfqVdxw6BCN+e8NP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurenx1/learning-systems/blob/main/final_regularized_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9tYE2mLK7MV",
        "outputId": "fbb157be-9daa-438a-df86-4738ed5ec1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading all the data\n",
        "train = np.loadtxt('/content/drive/MyDrive/final_features.train')\n",
        "test = np.loadtxt('/content/drive/MyDrive/final_features.test')\n",
        "\n",
        "\n",
        "# splitting into digit intensity symmetry\n",
        "digits = train[:, 0]\n",
        "intensity = train[:, 1]\n",
        "symmetry = train[:, 2]\n",
        "\n",
        "# constructing x_train and y_train\n",
        "x_train = np.column_stack((intensity, symmetry))\n",
        "y_train = digits\n",
        "\n",
        "digits2 = test[:, 0]\n",
        "intensity2 = test[:, 1]\n",
        "symmetry2 = test[:, 2]\n",
        "\n",
        "# constructing x_test and y_test\n",
        "x_test = np.column_stack((intensity2, symmetry2))\n",
        "y_test = digits2"
      ],
      "metadata": {
        "id": "qDFFhSEddNo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making our tranformed data, whether it be just adding a bias term or doing a non-linear transformation\n",
        "\n",
        "# just adds the bias term of 1 to the front\n",
        "def add_bias_term(X):\n",
        "  x1, x2 = X\n",
        "  return np.array([1, x1, x2])\n",
        "\n",
        "# non-linear tranform as described in question 8\n",
        "def feature_transform(X):\n",
        "  x1, x2 = X\n",
        "  return np.array([1, x1, x2, x1*x2, x1*x1, x2*x2])\n",
        "\n",
        "# function that actually applies the tranform to all the data\n",
        "def phi(X, transform_bool): # transform bool to determine which tranform is being done\n",
        "  if transform_bool:\n",
        "    Z = np.apply_along_axis(feature_transform, 1, X)\n",
        "    return Z\n",
        "  else:\n",
        "    Z = np.apply_along_axis(add_bias_term, 1, X)\n",
        "    return Z"
      ],
      "metadata": {
        "id": "cSmXCpLSRBwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating error, works for E_in or E_out, just switch out train or test data\n",
        "def calc_error(Z, W, Y, N):\n",
        "  pred = np.sign(np.dot(Z, W))\n",
        "  E = sum(pred != Y) / N\n",
        "  return E"
      ],
      "metadata": {
        "id": "2xvk0rA5Rf9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming the data and performing least-squares linear regression on Z\n",
        "def regularized_linear_regression(X, Y, lmbda):\n",
        "  identity_matrix = np.identity(X.shape[1])\n",
        "  W = np.linalg.inv(X.T @ X + lmbda * identity_matrix) @ X.T @ Y # the exact function given in the question\n",
        "  return W # returns a regularized weight vector"
      ],
      "metadata": {
        "id": "du0cltS6Vbnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing question 7\n",
        "\n",
        "def q7(x_train, y_train, lmbda, transform_bool, classifiers):\n",
        "  # Apply the transformation once before the loop\n",
        "  z_train = phi(x_train, transform_bool)\n",
        "\n",
        "  min_ein = float('inf')\n",
        "  best_classifier = None\n",
        "\n",
        "  for c in classifiers:\n",
        "    # Filter the data for the selected digit\n",
        "    y_digit_train = np.where(y_train == c, 1, -1)  # Set label +1 for the selected digit, -1 for others\n",
        "\n",
        "    # No need to apply the transformation here, use the pre-transformed data\n",
        "    w = regularized_linear_regression(z_train, y_digit_train, lmbda)\n",
        "    ein = calc_error(z_train, w, y_digit_train, len(y_digit_train))\n",
        "\n",
        "    print(f\"Ein for classifier {c} versus all:\", ein)\n",
        "\n",
        "    if ein < min_ein:\n",
        "      min_ein = ein\n",
        "      best_classifier = c\n",
        "\n",
        "  print(f\"\\nThe classifier with the lowest Ein is: {best_classifier} versus all, with Ein = {min_ein}\")\n",
        "\n",
        "\n",
        "# arguments as given to us in the problem\n",
        "lambda_value = 1\n",
        "transform_option = False  # would set to True for non-linear transform (like in question 8), False for bias term only\n",
        "classifiers_to_compare = [5, 6, 7, 8, 9]\n",
        "\n",
        "q7(x_train, y_train, lambda_value, transform_option, classifiers_to_compare)"
      ],
      "metadata": {
        "id": "dGspqeiGXUFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76bb73a7-9bef-4e8a-a97e-e016ed8150e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ein for classifier 5 versus all: 0.07625840076807022\n",
            "Ein for classifier 6 versus all: 0.09107118365107666\n",
            "Ein for classifier 7 versus all: 0.08846523110684405\n",
            "Ein for classifier 8 versus all: 0.07433822520916199\n",
            "Ein for classifier 9 versus all: 0.08832807570977919\n",
            "\n",
            "The classifier with the lowest Ein is: 8 versus all, with Ein = 0.07433822520916199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ein for classifier 5 versus all: 0.07625840076807022\n",
        "\n",
        "Ein for classifier 6 versus all: 0.09107118365107666\n",
        "\n",
        "Ein for classifier 7 versus all: 0.08846523110684405\n",
        "\n",
        "Ein for classifier 8 versus all: 0.07433822520916199\n",
        "\n",
        "Ein for classifier 9 versus all: 0.08832807570977919\n",
        "\n",
        "The classifier with the lowest Ein is: 8 versus all, with Ein = 0.07433822520916199"
      ],
      "metadata": {
        "id": "QrJxorTsBEfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing question 8\n",
        "def q8(x_train, y_train, x_test, y_test, lmbda, transform_bool, classifiers):\n",
        "  z_train = phi(x_train, transform_bool)\n",
        "  z_test = phi(x_test, transform_bool)\n",
        "\n",
        "  min_eout = float('inf')\n",
        "  best_classifier_1 = None\n",
        "\n",
        "  for c in classifiers:\n",
        "    y_digit_train = np.where(y_train == c, 1, -1)\n",
        "    y_digit_test = np.where(y_test == c, 1, -1)\n",
        "\n",
        "    w = regularized_linear_regression(z_train, y_digit_train, lmbda)\n",
        "    eout = calc_error(z_test, w, y_digit_test, len(y_digit_test))\n",
        "\n",
        "    print(f\"Eout for classifier {c} versus all:\", eout)\n",
        "\n",
        "    if eout < min_eout:\n",
        "      min_eout = eout\n",
        "      best_classifier_1 = c\n",
        "\n",
        "  print(f\"\\nThe classifier with the lowest Eout is: {best_classifier_1} versus all, with Eout = {min_eout}\")\n",
        "\n",
        "# args for 8\n",
        "lambda_value = 1\n",
        "to_transform = True\n",
        "classifiers_array = [0, 1, 2, 3, 4]\n",
        "\n",
        "q8(x_train, y_train, x_test, y_test, lambda_value, to_transform, classifiers_array)"
      ],
      "metadata": {
        "id": "emCsMdGOFxf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2daccf-698b-4d7f-d0dd-5be1057344d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eout for classifier 0 versus all: 0.10662680617837568\n",
            "Eout for classifier 1 versus all: 0.02192326856003986\n",
            "Eout for classifier 2 versus all: 0.09865470852017937\n",
            "Eout for classifier 3 versus all: 0.08271051320378675\n",
            "Eout for classifier 4 versus all: 0.09965122072745392\n",
            "\n",
            "The classifier with the lowest Eout is: 1 versus all, with Eout = 0.02192326856003986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eout for classifier 0 versus all: 0.10662680617837568\n",
        "\n",
        "Eout for classifier 1 versus all: 0.02192326856003986\n",
        "\n",
        "Eout for classifier 2 versus all: 0.09865470852017937\n",
        "\n",
        "Eout for classifier 3 versus all: 0.08271051320378675\n",
        "\n",
        "Eout for classifier 4 versus all: 0.09965122072745392\n",
        "\n",
        "The classifier with the lowest Eout is: 1 versus all, with Eout = 0.02192326856003986"
      ],
      "metadata": {
        "id": "nkleoMbET0S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q9(x_train, y_train, x_test, y_test, lmbda, classifiers):\n",
        "  q8(x_train, y_train, x_test,y_test, lmbda, False, classifiers)\n",
        "  q8(x_train, y_train, x_test, y_test, lmbda, True, classifiers)\n",
        "\n",
        "classifiers_9 = [0,1,2,3,4,5,6,7,8,9]\n",
        "l = 1\n",
        "q9(x_train, y_train, x_test, y_test, 1, classifiers_9)"
      ],
      "metadata": {
        "id": "_WUJb2aq-h1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e38846-c93d-4a59-bb0b-bbe10c6c5c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eout for classifier 0 versus all: 0.11509715994020926\n",
            "Eout for classifier 1 versus all: 0.02242152466367713\n",
            "Eout for classifier 2 versus all: 0.09865470852017937\n",
            "Eout for classifier 3 versus all: 0.08271051320378675\n",
            "Eout for classifier 4 versus all: 0.09965122072745392\n",
            "Eout for classifier 5 versus all: 0.07972097658196313\n",
            "Eout for classifier 6 versus all: 0.08470353761833582\n",
            "Eout for classifier 7 versus all: 0.07324364723467862\n",
            "Eout for classifier 8 versus all: 0.08271051320378675\n",
            "Eout for classifier 9 versus all: 0.08819133034379671\n",
            "\n",
            "The classifier with the lowest Eout is: 1 versus all, with Eout = 0.02242152466367713\n",
            "Eout for classifier 0 versus all: 0.10662680617837568\n",
            "Eout for classifier 1 versus all: 0.02192326856003986\n",
            "Eout for classifier 2 versus all: 0.09865470852017937\n",
            "Eout for classifier 3 versus all: 0.08271051320378675\n",
            "Eout for classifier 4 versus all: 0.09965122072745392\n",
            "Eout for classifier 5 versus all: 0.07922272047832586\n",
            "Eout for classifier 6 versus all: 0.08470353761833582\n",
            "Eout for classifier 7 versus all: 0.07324364723467862\n",
            "Eout for classifier 8 versus all: 0.08271051320378675\n",
            "Eout for classifier 9 versus all: 0.08819133034379671\n",
            "\n",
            "The classifier with the lowest Eout is: 1 versus all, with Eout = 0.02192326856003986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ein for classifier 0 versus all: 0.10931285146070498\n",
        "\n",
        "Ein for classifier 1 versus all: 0.01522424907420107\n",
        "\n",
        "Ein for classifier 2 versus all: 0.10026059525442327\n",
        "\n",
        "Ein for classifier 3 versus all: 0.09024825126868742\n",
        "\n",
        "Ein for classifier 4 versus all: 0.08942531888629818\n",
        "\n",
        "Ein for classifier 5 versus all: 0.07625840076807022\n",
        "\n",
        "Ein for classifier 6 versus all: 0.09107118365107666\n",
        "\n",
        "Ein for classifier 7 versus all: 0.08846523110684405\n",
        "\n",
        "Ein for classifier 8 versus all: 0.07433822520916199\n",
        "\n",
        "Ein for classifier 9 versus all: 0.08832807570977919\n",
        "\n",
        "The classifier with the lowest Ein is: 1 versus all, with Ein = 0.01522424907420107\n",
        "\n",
        "Eout for classifier 0 versus all: 0.10662680617837568\n",
        "\n",
        "Eout for classifier 1 versus all: 0.02192326856003986\n",
        "\n",
        "Eout for classifier 2 versus all: 0.09865470852017937\n",
        "\n",
        "Eout for classifier 3 versus all: 0.08271051320378675\n",
        "\n",
        "Eout for classifier 4 versus all: 0.09965122072745392\n",
        "\n",
        "Eout for classifier 5 versus all: 0.07922272047832586\n",
        "\n",
        "Eout for classifier 6 versus all: 0.08470353761833582\n",
        "\n",
        "Eout for classifier 7 versus all: 0.07324364723467862\n",
        "\n",
        "Eout for classifier 8 versus all: 0.08271051320378675\n",
        "\n",
        "Eout for classifier 9 versus all: 0.08819133034379671\n",
        "\n",
        "The classifier with the lowest Eout is: 1 versus all, with Eout = 0.02192326856003986\n"
      ],
      "metadata": {
        "id": "GxPsTcH8T9sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q10(x_train, y_train, x_test, y_test, lmbda_arr, transform_bool, class_pairs):\n",
        "    # Apply the transformation once before the loop for both training and test data\n",
        "  z_train = phi(x_train, transform_bool)\n",
        "  z_test = phi(x_test, transform_bool)\n",
        "\n",
        "  min_ein = float('inf')\n",
        "  min_eout = float('inf')\n",
        "  best_classifier = None\n",
        "  best_lambda = None\n",
        "\n",
        "  for lmbda in lmbda_arr:\n",
        "    for pair in class_pairs:\n",
        "      digit1, digit2 = pair\n",
        "\n",
        "      # Filter the training data for the selected digit pair\n",
        "      indices_train = np.where((y_train == digit1) | (y_train == digit2))\n",
        "      x_digit_train = x_train[indices_train]\n",
        "      y_digit_train = np.where(y_train[indices_train] == digit1, 1, -1)\n",
        "\n",
        "      # Filter the test data for the selected digit pair\n",
        "      indices_test = np.where((y_test == digit1) | (y_test == digit2))\n",
        "      x_digit_test = x_test[indices_test]\n",
        "      y_digit_test = np.where(y_test[indices_test] == digit1, 1, -1)\n",
        "\n",
        "      # No need to apply the transformation here, use the pre-transformed data\n",
        "      w = regularized_linear_regression(z_train[indices_train], y_digit_train, lmbda)\n",
        "\n",
        "      ein = calc_error(z_train[indices_train], w, y_digit_train, len(y_digit_train))\n",
        "      eout = calc_error(z_test[indices_test], w, y_digit_test, len(y_digit_test))\n",
        "\n",
        "      print(f\"Ein for classifier {digit1} versus {digit2} with lambda = {lmbda}: {ein}\")\n",
        "      print(f\"Eout for classifier {digit1} versus {digit2} with lambda = {lmbda}: {eout}\")\n",
        "\n",
        "      if ein < min_ein:\n",
        "        min_ein = ein\n",
        "        best_classifier = pair\n",
        "        best_lambda = lmbda\n",
        "\n",
        "      if eout < min_eout:\n",
        "        min_eout = eout\n",
        "\n",
        "  #print(f\"\\nThe best classifier with the lowest Ein is: {best_classifier} with lambda = {best_lambda} and Ein = {min_ein}\")\n",
        "  #print(f\"The best classifier with the lowest Eout is: {best_classifier} with lambda = {best_lambda} and Eout = {min_eout}\")\n",
        "\n",
        "\n",
        "\n",
        "lmbda_arr = [0.01, 1]\n",
        "classifier_pairs = [(1, 5)]\n",
        "transform = True\n",
        "q10(x_train, y_train, x_test, y_test, lmbda_arr, transform, classifier_pairs)\n"
      ],
      "metadata": {
        "id": "nszha_TgAYcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ein for classifier 1 versus 5 with lambda = 0.01: 0.004484304932735426\n",
        "\n",
        "Eout for classifier 1 versus 5 with lambda = 0.01: 0.02830188679245283\n",
        "\n",
        "Ein for classifier 1 versus 5 with lambda = 1: 0.005124919923126201\n",
        "\n",
        "Eout for classifier 1 versus 5 with lambda = 1: 0.025943396226415096"
      ],
      "metadata": {
        "id": "hkYP9GrMUJTp"
      }
    }
  ]
}